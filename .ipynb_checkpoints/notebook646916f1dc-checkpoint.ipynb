{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c113394afe5202f231882097fb8570c0ffc5ed1"
   },
   "outputs": [],
   "source": [
    "# Convert email into feature vector\n",
    "# Create Test & Training Set\n",
    "# Add Hyperparameters to:\n",
    "# - Strip email headers\n",
    "# - Convert to lowercase\n",
    "# - Remove punctuation\n",
    "# - Replace urls with \"URL\"\n",
    "# - Replace numbers with \"NUMBER\"\n",
    "# - Perform Stemming (trim word endings with library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10a6b5fbf97d211dc85df517ddc969252dc35641"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.listdir('../input/hamnspam/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb610ab3ed92e2fbf24180f8b9f184bd68c3d37d"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "834eeb7df8d3d1853ddfe762deed0289c275d4d3"
   },
   "outputs": [],
   "source": [
    "ham_filenames = [name for name in sorted(os.listdir('../input/hamnspam/ham')) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir('../input/hamnspam/spam')) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ab58266abce0cd7ce8f90458fe61216f3bda9d2"
   },
   "outputs": [],
   "source": [
    "print('Amount of ham files:', len(ham_filenames))\n",
    "print('Amount of spam files:', len(spam_filenames))    \n",
    "print('Spam to Ham Ratio:',len(spam_filenames)/len(ham_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f8f0e15859ab2495b7e31904e61bc38d3e8f046"
   },
   "outputs": [],
   "source": [
    "def load_email(is_spam, filename):\n",
    "    directory = \"../input/hamnspam/spam\" if is_spam else \"../input/hamnspam/ham\"\n",
    "    with open(os.path.join(directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "    \n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\n",
    "    \n",
    "    \n",
    "testEmail = ham_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f7d3ba2df081fdb34eb4ec18fe2a64a3bfb215e"
   },
   "outputs": [],
   "source": [
    "print('Header Field Names:',testEmail.keys())\n",
    "print('\\n\\n')\n",
    "print('Message Field Values:',testEmail.values())\n",
    "print('\\n\\n')\n",
    "print('Message Content:',testEmail.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0826b9646a9365039ab38a122447ed44a56c340d"
   },
   "outputs": [],
   "source": [
    "testEmailContent = testEmail.get_content()\n",
    "type(testEmailContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "620e5be77583d9b9d265406600a664878f633f1d"
   },
   "outputs": [],
   "source": [
    "testEmail['Subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9725b86918f37456e2000270118bf1fcc3fffdf6"
   },
   "outputs": [],
   "source": [
    "print(spam_emails[6].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ff869354025bfc3a761395bb2f05e042dedefd2"
   },
   "source": [
    "## Turning Emails into plaintext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7d0316301c936307c0bc82a8c03db264a5994d4"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures\n",
    "\n",
    "ham_structure = structures_counter(ham_emails)\n",
    "spam_structure = structures_counter(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a748abd9786756cfbafb8fd03eb5528adeea19ee"
   },
   "outputs": [],
   "source": [
    "ham_structure.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ed57dddcd8e2051716ab631316dd1e2083ec7e5"
   },
   "outputs": [],
   "source": [
    "spam_structure.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6e9ee88905a3fa66d14892c9e2e9bec5844ece2"
   },
   "outputs": [],
   "source": [
    "for email in spam_emails:\n",
    "    if get_email_structure(email) == 'text/html':\n",
    "        testEmail = email\n",
    "        break\n",
    "\n",
    "print(testEmail.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3dc527c21200e17078aecf0c934581d1ed504d7b"
   },
   "outputs": [],
   "source": [
    "def html_to_plain(email):\n",
    "    try:\n",
    "        soup = BeautifulSoup(email.get_content(), 'html.parser')\n",
    "        return soup.text.replace('\\n\\n','')\n",
    "    except:\n",
    "        return \"empty\"\n",
    "\n",
    "print(html_to_plain(testEmail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01754112c23525e775e4ec41b42f591947c7ee82"
   },
   "outputs": [],
   "source": [
    "def email_to_plain(email):\n",
    "    struct = get_email_structure(email)\n",
    "    for part in email.walk():\n",
    "        partContentType = part.get_content_type()\n",
    "        if partContentType not in ['text/plain','text/html']:\n",
    "            continue\n",
    "        try:\n",
    "            partContent = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            partContent = str(part.get_payload())\n",
    "        if partContentType == 'text/plain':\n",
    "            return partContent\n",
    "        else:\n",
    "            return html_to_plain(part)\n",
    "\n",
    "print(email_to_plain(ham_emails[42]))\n",
    "print(email_to_plain(spam_emails[42]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1b121498607570ea196a5ae9223ffeea18e06cf"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "for word in (\"Working\", \"Work\", \"Works\", \"Worked\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "342aefbcb71b3fd45e987984de7e312be26646f7"
   },
   "outputs": [],
   "source": [
    "#import url_extractor\n",
    "#url_extractor = urlextract.URLExtract()\n",
    "#print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pqs\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eac3df504e5a4f67af8342a5a6899d71d560b4a1"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# - Strip email headers\n",
    "# - Convert to lowercase\n",
    "# - Remove punctuation\n",
    "# - Replace urls with \"URL\"\n",
    "# - Replace numbers with \"NUMBER\"\n",
    "# - Perform Stemming (trim word endings with library)\n",
    "class EmailToWords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stripHeaders=True, lowercaseConversion = True, punctuationRemoval = True, \n",
    "                 urlReplacement = True, numberReplacement = True, stemming = True):\n",
    "        self.stripHeaders = stripHeaders\n",
    "        self.lowercaseConversion = lowercaseConversion\n",
    "        self.punctuationRemoval = punctuationRemoval\n",
    "        self.urlReplacement = urlReplacement\n",
    "        #self.url_extractor = urlextract.URLExtract()\n",
    "        self.numberReplacement = numberReplacement\n",
    "        self.stemming = stemming\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_to_words = []\n",
    "        for email in X:\n",
    "            text = email_to_plain(email)\n",
    "            if text is None:\n",
    "                text = 'empty'\n",
    "            if self.lowercaseConversion:\n",
    "                text = text.lower()\n",
    "                \n",
    "            #if self.urlReplacement:\n",
    "                #urls = self.url_extractor.find_urls(text)\n",
    "                #for url in urls:\n",
    "                #    text = text.replace(url, 'URL')   \n",
    "                    \n",
    "            if self.punctuationRemoval:\n",
    "                text = text.replace('.','')\n",
    "                text = text.replace(',','')\n",
    "                text = text.replace('!','')\n",
    "                text = text.replace('?','')\n",
    "                \n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming:\n",
    "                stemmed_word_count = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = self.stemmer.stem(word)\n",
    "                    stemmed_word_count[stemmed_word] += count\n",
    "                word_counts = stemmed_word_count\n",
    "            X_to_words.append(word_counts)\n",
    "        return np.array(X_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3af67dc03e4573264b4bff9fee8ce86576a38e17"
   },
   "outputs": [],
   "source": [
    "X_few = ham_emails[:3]\n",
    "Xwordcounts = EmailToWords().fit_transform(X_few)\n",
    "Xwordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05aa0f5c49646b3c807eb7510ec7a373264aebfe"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCountToVector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_word_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_word_count[word] += min(count, 10)\n",
    "        self.most_common = total_word_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(self.most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0792afdbb7b3a82d986a2af29ce6201e171b152"
   },
   "outputs": [],
   "source": [
    "vocab_transformer = WordCountToVector(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(Xwordcounts)\n",
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7317205a32bedbc7283826eb376857784eb4570d"
   },
   "outputs": [],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d8d99f62ece95ccbb202f01599adcb7c88113c4"
   },
   "source": [
    "## Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7f8f68d2e235f4e3cd0766e74b750ad7a25643a"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "email_pipeline = Pipeline([\n",
    "    (\"Email to Words\", EmailToWords()),\n",
    "    (\"Wordcount to Vector\", WordCountToVector()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4610dc1a30c12f9a8cb14e3899478ba6b7509a5"
   },
   "source": [
    "## Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9907721a6f58eafd80b948f06e8599dc919dabb4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9b7d1dce50cd38147eff76cfcb47c77c5fe64f1"
   },
   "outputs": [],
   "source": [
    "X_augmented_train = email_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46ebbe3023e450039a21f623da078dcfb23f03cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_augmented_train, y_train, cv=3)\n",
    "score.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31ecf27b66649ad32b9dfcb83f76f165597eb3df"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_augmented_test = email_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_augmented_train, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_augmented_test)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
